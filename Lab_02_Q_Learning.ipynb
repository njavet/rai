{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm\n",
    "\n",
    "This assignment will review the Q-learning algorithm and the e-greedy approach. You have to update the Qlearning class below to implement these algorithms. The parts of code that need to be changed as labelled as TODOs in the comments.\n",
    "\n",
    "All your answers should be written in this notebook. You shouldn’t need to write or modify any other files. You should execute every block of code to not miss any dependency.\n",
    "\n",
    "At the end of the notebook you will find a set of questions to test your understanding of the algorithm.\n",
    "\n",
    "The experiments use the [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "environment from the reinforcement learning [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "![frozenlake](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies & Parameters\n",
    "\n",
    "Here we import some needed libraries and define the parameters of the Frozenlake environment and the learning process.\n",
    "\n",
    "Notice that it also creates a directory to store the created figures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "from misc import *\n",
    "\n",
    "sns.set_theme()\n",
    "%matplotlib inline\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder where plots are saved\n",
    "\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=2000,\n",
    "    learning_rate=0.8,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=5,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"./_static/img\"),\n",
    ")\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exist\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The frozen lake environment\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qlearning class - UPDATE HERE TO IMPLEMENT Q-LEARNING AND e-GREEDY\n",
    "\n",
    "In this implementation we use Tabular Q-learning as our learning algorithm and\n",
    "$\\epsilon$-greedy to decide which action to pick at each step.\n",
    "\n",
    "The **Action-value** function (Q(s,a)) is implemented as a table storing the expected\n",
    "return for each state,action pair: `qtable[state,action])`. \n",
    "\n",
    "We first create our Q-table initialized at zero with the states number as rows and the actions number as columns.\n",
    "\n",
    "Then, you need to modify `update` to allow the updating of the Q(s,a) function (i.e., the table) according to the Q-learning algorithm, and the `EpsilonGreedy` to allow for the system to exhibit both exploration and exploitation behaviors\n",
    "\n",
    "You can have a look at the slides or the references below for some refreshers on the theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "print(f\"Action size: {params.action_size}\")\n",
    "print(f\"State size: {params.state_size}\")\n",
    "\n",
    "\n",
    "class Qlearning:\n",
    "    def __init__(self, learning_rate, gamma, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.reset_qtable()\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        \"\"\"TODO: Change the following code to implement the update of the Q-function\n",
    "            Q_update(s,a):= Q(s,a) + learning_rate * delta\n",
    "                delta =  [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\"\"\"\n",
    "\n",
    "        q_update = self.qtable[state, action]\n",
    "        return q_update\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        \"\"\"Reset the Q-table.\"\"\"\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, action_space, state, qtable):\n",
    "        \"\"\"TODO: Implement the e-greedy algorithm. i.e.:\n",
    "            with probability epsilon:\n",
    "                select an action randomly\n",
    "            else\n",
    "                select the action with the highest q-value\"\"\"\n",
    "\n",
    "        # Select a random action\n",
    "        action = action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the learner and the explorer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Qlearning(\n",
    "    learning_rate=params.learning_rate,\n",
    "    gamma=params.gamma,\n",
    "    state_size=params.state_size,\n",
    "    action_size=params.action_size,\n",
    ")\n",
    "explorer = EpsilonGreedy(\n",
    "    epsilon=params.epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Running the environment without any learnning\n",
    "\n",
    "The code below executes all the steps for the agent to interact with the environment, except calling the learning function. As a result, it tests how does an agent with a random policy and no learning would behave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env_no_learning():\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # Run several times to account for stochasticity\n",
    "        \n",
    "        for episode in tqdm(\n",
    "            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "        ):\n",
    "            state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = explorer.choose_action(\n",
    "                    action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "                )\n",
    "\n",
    "                # Log all states and actions\n",
    "                all_states.append(state)\n",
    "                all_actions.append(action)\n",
    "\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                total_rewards += reward\n",
    "                step += 1\n",
    "\n",
    "                # Our new state is state\n",
    "                state = new_state\n",
    "\n",
    "            # Log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "        qtables[run, :, :] = learner.qtable\n",
    "\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates an agent with a random policy, runs the episodes and plots the outcome of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "all_actions = []\n",
    "\n",
    "env = gym.make(\n",
    "\"FrozenLake-v1\",\n",
    "is_slippery=params.is_slippery,\n",
    "render_mode=\"rgb_array\",\n",
    "desc=generate_random_map(\n",
    "    size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "),\n",
    ")\n",
    "\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "\n",
    "env.action_space.seed(\n",
    "params.seed\n",
    ")  # Set the seed to get reproducible results when sampling the action space\n",
    "learner = Qlearning(\n",
    "learning_rate=params.learning_rate,\n",
    "gamma=params.gamma,\n",
    "state_size=params.state_size,\n",
    "action_size=params.action_size,\n",
    ")\n",
    "\n",
    "# Force exploration in every step \n",
    "explorer = EpsilonGreedy(\n",
    "    epsilon=1.0,\n",
    ")\n",
    "\n",
    "state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "\n",
    "# Initialize the Q-table with zero values\n",
    "learner.reset_qtable()\n",
    "\n",
    "# Generate several trajectories using the Q-function without exploration and plot\n",
    "# the distribution of states visited and actions taken\n",
    "\n",
    "# Notice that if the policy directs the agent towards the border of the \n",
    "# environment, it will remain stuck at that point\n",
    "\n",
    "rewards, steps, episodes, qtables, all_states, all_actions = run_env_no_learning()\n",
    "\n",
    "# Save the results in dataframes\n",
    "res, st = postprocess(episodes, params, rewards, steps, params.map_size)\n",
    "\n",
    "# Plot the Q-table\n",
    "plot_q_values_map(learner.qtable, env, params.map_size, params, img_label='random')\n",
    "\n",
    "# Plot the state and action distribution\n",
    "plot_states_actions_distribution(\n",
    "    states=all_states, actions=all_actions, map_size=params.map_size, params=params, img_label='random'\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps_and_rewards(res, st, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's apply now the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the environment to learn a strategy\n",
    "\n",
    "This will be our main function to run our environment and learn the Q-function until the maximum\n",
    "number of episodes ``params.total_episodes``. To account for\n",
    "stochasticity, we will also run our environment a few times.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env():\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # Run several times to account for stochasticity\n",
    "        learner.reset_qtable()  # Reset the Q-table between runs\n",
    "\n",
    "        for episode in tqdm(\n",
    "            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "        ):\n",
    "            state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = explorer.choose_action(\n",
    "                    action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "                )\n",
    "\n",
    "                # Log all states and actions\n",
    "                all_states.append(state)\n",
    "                all_actions.append(action)\n",
    "\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                learner.qtable[state, action] = learner.update(\n",
    "                    state, action, reward, new_state\n",
    "                )\n",
    "\n",
    "                total_rewards += reward\n",
    "                step += 1\n",
    "\n",
    "                # Our new state is state\n",
    "                state = new_state\n",
    "\n",
    "            # Log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "        qtables[run, :, :] = learner.qtable\n",
    "\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explore the environment to learn the Value function using Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")\n",
    "\n",
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "env.action_space.seed(\n",
    "    params.seed\n",
    ")  # Set the seed to get reproducible results when sampling the action space\n",
    "learner = Qlearning(\n",
    "    learning_rate=params.learning_rate,\n",
    "    gamma=params.gamma,\n",
    "    state_size=params.state_size,\n",
    "    action_size=params.action_size,\n",
    ")\n",
    "explorer = EpsilonGreedy(\n",
    "    epsilon=params.epsilon,\n",
    ")\n",
    "\n",
    "rewards, steps, episodes, qtables, all_states, all_actions = run_env()\n",
    "\n",
    "# Save the results in dataframes\n",
    "res, st = postprocess(episodes, params, rewards, steps, params.map_size)\n",
    "qtable = qtables.mean(axis=0)  # Average the Q-table between runs\n",
    "\n",
    "# Plot the Q-table\n",
    "plot_q_values_map(qtable, env, params.map_size, params=params, img_label='learned')\n",
    "\n",
    "# Plot the state and action distribution\n",
    "plot_states_actions_distribution(\n",
    "    states=all_states, actions=all_actions, map_size=params.map_size, params=params, img_label='learned'\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if our agent is learning, we can plot the cumulated sum of\n",
    "rewards, as well as the number of steps needed until the end of the\n",
    "episode. If our agent is learning, we expect to see the cumulated sum of\n",
    "rewards to increase and the number of steps to solve the task to\n",
    "decrease.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_steps_and_rewards(res, st, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "1. Is the agent able to learn the right policy using Q-learning?\n",
    "2. Why does the cumulated reward increases even when we use a random policy and no learning? \n",
    "3. Evaluate different values of the alpha, gamma and epsilon hyperparameters to assess their effect in the learning\n",
    "4. Change the size of the environment to 11x11 and compare the learning process\n",
    "5. Change the parameter is_slippery to TRUE to see how it impacts the learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "-  Code inspired from [Deep Reinforcement Learning\n",
    "   Course](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)_\n",
    "   by Thomas Simonini (http://simoninithomas.com/)\n",
    "-  [Dissecting Reinforcement\n",
    "   Learning-Part.2](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html)_\n",
    "-  [Dadid Silver’s course](https://www.davidsilver.uk/teaching/)_ in\n",
    "   particular lesson 4 and lesson 5\n",
    "-  [Q-learning article on\n",
    "   Wikipedia](https://en.wikipedia.org/wiki/Q-learning)_\n",
    "-  [Q-Learning: Off-Policy TD\n",
    "   Control](http://incompleteideas.net/book/ebook/node65.html)_ in\n",
    "   [Reinforcement Learning: An Introduction, by Richard S. Sutton and\n",
    "   Andrew G. Barto](http://incompleteideas.net/book/ebook/)_\n",
    "-  [Epsilon-Greedy\n",
    "   Q-learning](https://www.baeldung.com/cs/epsilon-greedy-q-learning)_\n",
    "-  [Introduction to Reinforcement\n",
    "   Learning](https://gibberblot.github.io/rl-notes/index.html)_ by Tim\n",
    "   Miller (University of Melbourne)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
