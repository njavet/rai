{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbhF6-H5x_b7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDr7ZGY-x_b9"
   },
   "source": [
    "# Q-Learning on the CartPole Environment\n",
    "\n",
    "This week we introduce a new environment: CartPole-v1 task from the [OpenAI Gym](https://gym.openai.com/).\n",
    "\n",
    "![cartpole](https://github.com/pytorch/tutorials/blob/main/_static/img/cartpole.gif?raw=true)\n",
    "\n",
    "In this environment, the task is to balance the pole that is attached to the cart, by moving the cart to either side. The reward gets incremented for each step (for up to 200 steps) where the pole is not exceeding a set angle and the cart is not touching the sides of the line.\n",
    "\n",
    "The environment provides four parameters that represent the state of the environment:\n",
    "Position and velocity of the cart, and the angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
    "\n",
    "In this notebook, we will implement Q-learning for this environment. As you will notice, the state space of Cartpole uses continues values, hence we will need to discretize, in order to apply the tabular version of Q-Learning.\n",
    "\n",
    "This practice is based on a post by Jose Nieves Flores Maynez ([link](https://medium.com/@flomay/using-q-learning-to-solve-the-cartpole-balancing-problem-c0a7f47d3f9d)) and code from Isaac Patole([link](https://github.com/init-22/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCWbuk66234H"
   },
   "source": [
    "### Packages\n",
    "\n",
    "\n",
    "First, let's import needed packages. You may need to install 'gym', 'seaborn' and other packages. You can use pip or conda for this purpose.\n",
    "\n",
    "**Note: For compatibility reason, if you are using your own laptop ensure that you are use the gym version 0.25.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ccdmXWUx_b_"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import NamedTuple\n",
    "import seaborn as sns\n",
    "# additional imports for saving and loading a trained policy\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "print(f'gym.__version__ = {gym.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psxbGvWJx_cB"
   },
   "source": [
    "## Implementating the Cartpole agent\n",
    "\n",
    "The CartPole environment gives us the state of the agent in terms of the position of the cart, its velocity, the angle of the pole and the velocity at the tip of the pole. However, all of these are continuous variables. \n",
    "\n",
    "To be able to solve this problem, we need to discretize these states. The solution is to group several values of each of the variables into the same “bucket” and treat them as similar states. \n",
    "\n",
    "The tabular Q-learning encodes the **Action-value** function (Q(s,a)) as a table storing the expected\n",
    "return for each state,action pair: `Q_table[state,action])`. \n",
    "\n",
    "The code below allows to have a variable learning rate and $\\epsilon$, defined by the decay parameter. Typically, we encourage higher exploration and learning rates at the beginning of the interaction and, as the agent learns better policies, these values decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first implement a class to set the learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    seed: float  # Define a seed so that we get reproducible results\n",
    "    buckets_pos: int # Number of buckets for discretizing Cart Position\n",
    "    buckets_vel: int # Number of buckets for discretizing Cart Velocity\n",
    "    buckets_ang: int #  Number of buckets for discretizing Pole Angle\n",
    "    buckets_angV: int #  Number of buckets for discretizing Pole Angular Velocity\n",
    "    num_episodes: int \n",
    "    min_lr: float # Min Lerning rate\n",
    "    min_epsilon: float # minimal epsilon\n",
    "    gamma: float # discount factor\n",
    "    decay_epsilon: float # decay for exploration parameter epsilon\n",
    "    decay_lr: float # decay for learning rate\n",
    "\n",
    "params = Params(\n",
    "    seed = 123,\n",
    "    buckets_pos = 3,\n",
    "    buckets_vel = 3,\n",
    "    buckets_ang = 6,\n",
    "    buckets_angV = 6,\n",
    "    num_episodes=500,\n",
    "    min_lr=0.1,\n",
    "    min_epsilon=0.1,\n",
    "    gamma=1.0,\n",
    "    decay_epsilon=25,\n",
    "    decay_lr=25)\n",
    "\n",
    "\n",
    "# Set the seed for the random number generator\n",
    "rng = np.random.default_rng(params.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the `CartPoleQAgent` class comprising the Q-learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "tqqVqJYZwjQr",
    "outputId": "547fb01b-c4d7-4c1f-b150-1e9e538a045c"
   },
   "outputs": [],
   "source": [
    "class CartPoleQAgent():\n",
    "    def __init__(self, env, buckets=(3, 3, 6, 6),\n",
    "                 num_episodes=500, min_lr=0.1,\n",
    "                 min_epsilon=0.1, gamma=1.0, decay_epsilon=25, decay_lr=25):\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.decay_epsilon = decay_epsilon\n",
    "        self.decay_lr = decay_lr\n",
    "\n",
    "        \n",
    "        # Initialize the action-value function\n",
    "        self.Q_table = np.zeros(self.buckets + (env.action_space.n,))\n",
    "\n",
    "        # Define bound values for the state variables [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "\n",
    "        # Create array to store the number of steps per episode\n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "\n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        \"\"\"\n",
    "        Takes an observation of the environment and discretizes it.\n",
    "        By doing this, very similar observations can be treated\n",
    "        as the same and it reduces the state space so that the\n",
    "        Q-table can be smaller and more easily filled.\n",
    "\n",
    "        Input:\n",
    "        obs (tuple): Tuple containing 4 floats describing the current\n",
    "                     state of the environment.\n",
    "\n",
    "        Output:\n",
    "        discretized (tuple): Tuple containing 4 non-negative integers smaller\n",
    "                             than n where n is the number in the same position\n",
    "                             in the buckets list.\n",
    "        \"\"\"\n",
    "        discretized = list()\n",
    "        \"\"\"\n",
    "        TODO: Complete the code to implement the discretization process. It should take the 4 values inthe agent state (obs)\n",
    "                and discretize them into non-negative integers corresponding to the respective bucket \n",
    "        \"\"\"\n",
    "        return tuple(discretized)\n",
    "\n",
    "    \n",
    "    def choose_action(self, env, state):\n",
    "        \"\"\"\n",
    "        Implementation of e-greedy algorithm. Returns an action (0 or 1).\n",
    "\n",
    "        Input:\n",
    "        state (tuple): Tuple containing 4 non-negative integers within\n",
    "                       the range of the buckets.\n",
    "\n",
    "        Output:\n",
    "        (int) Returns either 0 or 1\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        TODO: Implement the e-greedy algorithm. i.e.:\n",
    "            with probability epsilon:\n",
    "                select an action randomly\n",
    "            else\n",
    "                select the action with the highest q-value\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Updates Q-table (self.Q_table)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        TODO: Implement the update of the Q-function\n",
    "            Q(s,a):= Q(s,a) + learning_rate * delta\n",
    "                delta =  [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
    "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay_epsilon)))\n",
    "\n",
    "    \n",
    "    def get_learning_rate(self, t):\n",
    "        \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "        # Learning rate also declines as we add more episodes\n",
    "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay_lr)))\n",
    "\n",
    "    def train(self, env):\n",
    "        \"\"\"\n",
    "        Trains agent making it go through the environment and choose actions\n",
    "        through an e-greedy policy and updating values for its Q-table. The\n",
    "        agent is trained by default for 500 episodes with a declining\n",
    "        learning rate and epsilon values that with the default values,\n",
    "        reach the minimum after 198 episodes.\n",
    "        \"\"\"\n",
    "        # Looping for each episode\n",
    "        for e in range(self.num_episodes):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(list(env.reset()))\n",
    "\n",
    "            # Get new values of learning rate and epsilo (according to decay function)\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            \n",
    "            # Looping for each step\n",
    "            while not done:\n",
    "                self.steps[e] += 1\n",
    "                # Choose A from S\n",
    "                action = self.choose_action(env, current_state)\n",
    "                # Take action\n",
    "                obs, reward, terminated, truncated,_ = env.step(action)\n",
    "                done = truncated or terminated \n",
    "                new_state = self.discretize_state(obs)\n",
    "                # print (\"current_state\" + str(current_state) + \"new_state\" + str(new_state) +\"\\r\", flush = False)\n",
    "               \n",
    "                # Update Q(S,A) and new state\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "\n",
    "                # We break out of the loop when done is False which is\n",
    "                # a terminal state.\n",
    "\n",
    "        print('Finished training!')\n",
    "\n",
    "\n",
    "    def run(self, env, strname, num_episodes = 10):\n",
    "        \"\"\"Runs an episode and save videos of the cartpole environment in directory \"video_eval'.\"\"\"\n",
    "        self.epsilon = self.min_epsilon\n",
    "        t = 0\n",
    "        \n",
    "        envVid = RecordVideo(env, './video_eval/', name_prefix=strname, episode_trigger = lambda episode_number: True, new_step_api= True)\n",
    "        envVid.reset()\n",
    "        # Looping for several episodes\n",
    "        for e in range(num_episodes):\n",
    "            current_state = self.discretize_state(envVid.reset())\n",
    "            done = False\n",
    "            while not done:\n",
    "                    envVid.render()\n",
    "                    t = t+1\n",
    "                    action = self.choose_action(envVid, current_state)\n",
    "                    obs, reward, terminated, truncated, _ = envVid.step(action)\n",
    "                    done = truncated or terminated \n",
    "                    new_state = self.discretize_state(obs)\n",
    "                    current_state = new_state\n",
    "                \n",
    "        envVid.env.close()\n",
    "\n",
    "\n",
    "    \n",
    "    def plot_learning(self):\n",
    "        \"\"\"\n",
    "        Plots the number of steps at each episode and prints the\n",
    "        amount of times that an episode was successfully completed.\n",
    "        \"\"\"\n",
    "        sns.lineplot(data=self.steps)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.show()\n",
    "        t = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            if self.steps[i] >= 200:\n",
    "                t+=1\n",
    "        print(t, \"episode(s) were successfully completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agent and save videos of the untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment. We are creating the environment with 'render_mode=\"rgb_array\"' to save videos \n",
    "# of the policy behavior before training\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
    "env.reset(seed=params.seed)\n",
    "\n",
    "# Create an Agent\n",
    "agent = CartPoleQAgent(env,\n",
    "                       (params.buckets_pos, params.buckets_vel, params.buckets_ang, params.buckets_angV),\n",
    "                       params.num_episodes,\n",
    "                       params.min_lr,\n",
    "                       params.min_epsilon,\n",
    "                       params.gamma,\n",
    "                       params.decay_epsilon,\n",
    "                       params.decay_lr)\n",
    "\n",
    "\n",
    "# Save videos of the behavior of the untrained agent\n",
    "agent.run(env,\"before\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent and plot the number of steps per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have closed the environment and created a new one without rendering to speed up the training process\n",
    "env = gym.make('CartPole-v1', new_step_api=True)\n",
    "env.reset(seed=params.seed)\n",
    "\n",
    "agent.train(env)\n",
    "agent.plot_learning()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save videos with the behavior of the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, we create the CartPole Environment with rendering mode activated\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
    "env.reset(seed=params.seed)\n",
    "\n",
    "agent.run(env,\"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. **Where is the Q-Learning algorithm?**:\n",
    "    Find out where the training is happening.\n",
    "    Implement the functions to discretize the state space, to update the Q-value, and to choose the actions. For the last two functions, can you use the same expressions you used for the Gridworld environment?\n",
    "    \n",
    "2. **Extend the plotting**:\n",
    "    Implement a function to plot both the learning rate and the $\\epsilon$ parameter over the course of the training.\n",
    "\n",
    "3. **Discretization**:\n",
    "    Change the number of buckets used for discretization. What is the effect on learning?\n",
    "\n",
    "\n",
    "4. **Initial conditions**:\n",
    "    Adapt the code to run the training process 10 times with different seends of the random number generator. Does the learning process and outcome change across runs?\n",
    "\n",
    "\n",
    "5. **Update the code**:\n",
    "    Update the code to use a fixed learning rate to 0.1 and retrain. How does the learning changes compared with the initial implementation?\n",
    "    Now, do the same with the $\\epsilon$ parameter.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
