{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Work - Reinforcement Learning\n",
    "## Policy Learning on the CartPole Environment - SOLUTION\n",
    "\n",
    "*All your answers should be written in this notebook. You shouldn’t need to write or modify any other files. The parts of code that need to be changed as labelled as TODOs in the comments. At the end of the notebook you will find a set of questions to answer. You should execute every block of code to not miss any dependency.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "This practice applies the Deep Q Learning (DQN) algorithm to the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment  from [Gymnasium](https://www.gymnasium.farama.org). In this environment, the task is to balance the pole that is attached to the cart, by moving the cart to either side. The reward gets incremented for each step (for up to 200 steps) where the pole is not exceeding a set angle and the cart is not touching the sides of the line.\n",
    "\n",
    "## Environment\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find more\n",
    "information about the environment and other more challenging environments at\n",
    "[Gymnasium's website](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "![cartpole](./_static/img/cartpole.gif)\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more than 2.4\n",
    "units away from center. This means better performing scenarios will run\n",
    "for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.).\n",
    "We take these 4 inputs without any scaling and pass them through a\n",
    "small fully-connected network with 2 outputs, one for each action.\n",
    "The network is trained to predict the expected value for each action,\n",
    "given the input state. The action with the highest expected value is\n",
    "then chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2MqEqos04rH"
   },
   "source": [
    "# Policy gradients\n",
    "\n",
    "Policy gradients is a family of algorithms for solving reinforcement learning problems by directly optimizing the policy in policy space. This is in stark contrast to value based approaches (such as Q-learning used previous practices or in [Learning Atari games by DeepMind](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning). This practice implements the Vanilla Policy Gradient (also known as REINFORCE).\n",
    "\n",
    "Policy gradients have several appealing properties, for one they produce stochastic policies (by learning a probability distribution over actions given observations) whereas value based approaches are deterministic as they will typically choose actions greedily with respect to the value function being learned which can lead to under-exploration (one needs to introduce exploration strategies such as $\\epsilon$-greedy explicitly to get around this). Another advantage of policy gradients is their ability to tackle continuous action spaces without discretisation which is necessary for value based methods. \n",
    "\n",
    "One of the biggest disadvantages of policy gradients is their high variance estimates of the gradient updates. This leads to very noisy gradient estimates and can de-stabilize the learning process. Significant efforts in reinforcement learning research with policy gradients in the past few years has been about trying to reduce the variance of these gradient updates to improve the trainability of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving the policy gradient update\n",
    "\n",
    "The derivation of the policy gradient update is standard and can be found online in various levels of detail ([Karpathy](https://karpathy.github.io/2016/05/31/rl/),  [Takeshi](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/), [OpenAI](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient), [Abbeel](https://drive.google.com/file/d/0BxXI_RttTZAhY216RTMtanBpUnc/view)), and is included here for completeness.\n",
    "\n",
    "Let $\\tau=(s_0,a_0,\\dots,s_{T-1},a_{T-1}, s_T)$ be a state-action sequence in a complete trajectory consisting of $T$ steps (note, the final state $s_T$ is a terminal state which results from taking the final action $a_{T-1}$, after which the environment is reset). Define $R(s_t,a_t)$ to be the reward received after observing the state $s_t$ and performing an action $a_t$. Also define the (discounted)[^1] sum of these rewards to be $R(\\tau) := \\sum_{t=0}^{T-1}\\gamma^t R(s_t,a_t)$. Then our goal is to maximize the expected reward\n",
    "\n",
    "$$\n",
    "\\max_\\theta\\mathbb{E}_{\\pi_\\theta}R(\\tau),\n",
    "$$\n",
    "\n",
    "where $\\pi_\\theta$ is a parametrized policy (typically a neural network). The expected value is taken with respect to drawing trajectories $\\tau$ under the policy $\\pi_\\theta$ and so solving this problem is equivalent to finding the \"best\" parameters $\\theta$ that give the best policy for maximizing the expected reward.\n",
    "\n",
    "We can do this via the usual gradient ascent, i.e. suppose we know how to calculate the gradient with respect to the parameters, $\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta}R(\\tau)$, then we can update the parameters $\\theta$ in the direction of the gradient:\n",
    "\n",
    "$$\n",
    "\\theta\\leftarrow\\theta+\\alpha\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta}R(\\tau),\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the usual learning rate hyperparameter.\n",
    "\n",
    "Let $P(\\tau\\vert\\theta)$ be the probability of a trajectory $\\tau$ under the policy $\\pi_\\theta$. Then we can write the gradient as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta}R(\\tau) &= \\\\\n",
    "&= \\nabla_\\theta\\sum_\\tau P(\\tau\\vert\\theta)R(\\tau) & \\text{ definition of expectation}  \\\\\n",
    "&= \\sum_{\\tau}\\nabla_\\theta P(\\tau\\vert\\theta)R(\\tau) & \\text{ swap sum/integral and gradient} \\\\\n",
    "&= \\sum_{\\tau}\\frac{P(\\tau\\vert\\theta)}{P(\\tau\\vert\\theta)}\\nabla_\\theta P(\\tau\\vert\\theta)R(\\tau) & \\text{ multiply and divide by $P(\\tau\\vert\\theta)$} \\\\\n",
    "&= \\sum_{\\tau}P(\\tau\\vert\\theta)\\nabla_\\theta\\log P(\\tau\\vert\\theta)R(\\tau) & \\text{ recognize that $\\nabla_x\\log(f(x))=\\frac{\\nabla_x f(x)}{f(x)}$}\\\\\n",
    "&= \\mathbb{E}_{\\pi_\\theta}\\left(\\nabla_\\theta\\log P(\\tau\\vert\\theta)R(\\tau)\\right) & \\text{ definition of expectation}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can expand the probability of a trajectory $\\tau$ as follows:\n",
    "\n",
    "$$\n",
    "P(\\tau\\vert\\theta)=p(s_0)\\prod_{t=0}^{T-1}p(s_{t+1}\\vert s_t,a_t)\\pi_\\theta(a_t\\vert s_t),\n",
    "$$\n",
    "\n",
    "where $p(s_{t+1}\\vert s_t,a_t)$ is the probability of transitioning to state $s_{t+1}$ by taking an action $a_t$ in state $s_t$ (as specified by the Markov Decision Process underlying the RL problem) and $p(s_0)$ is the starting state distribution. Taking the gradient of the log-probability (abbreviated as grad-log-prob from here on) of a trajectory thus gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\label{eq:pg}\n",
    "\\nabla_\\theta\\log P(\\tau\\vert\\theta) &= \\\\\n",
    "&= \\nabla_\\theta\\left(\\log p(s_0)+\\sum_{t=0}^{T-1}\\left(\\log p(s_{t+1}\\vert s_t,a_t)+\\log\\pi_\\theta(a_t\\vert s_t)\\right)\\right) \\\\\n",
    "&= \\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note that after taking the gradient, the dynamics model $p(s_{t+1}\\vert s_t,a_t)$ disappears, i.e. policy gradients are a *model-free* method.\n",
    "\n",
    "Putting this all together we arrive at the policy gradient expression:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta} R(\\tau) = \\mathbb{E}_{\\pi_\\theta}\\left(\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)R(\\tau)\\right),\n",
    "$$\n",
    "and because it is an expectation it can be estimated by Monte Carlo sampling of trajectories:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta} R(\\tau)\\approx\\frac{1}{L}\\sum_{\\tau}\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)R(\\tau),\n",
    "$$\n",
    "where $L$ is the number of trajectories used for one gradient update.\n",
    "\n",
    "Equation (4) is the basic expression for a policy gradient update relating the observed rewards to the probabilities of the policy used to obtain them. However, if you look at it and think about the role of the rewards of the whole trajectory $R(\\tau)$ you notice something strange - it features as a multiplier for every grad-log-prob term in the sum over the episode, i.e. the reward obtained *over the whole episode* is used to change the probabilities of each action taken during the episode. This is saying that even rewards obtained before taking a certain action affect that action via the gradient update. This is not desirable as ideally we want to attribute to an action only those (and future) rewards that were received after taking that action.\n",
    "\n",
    "With some algebra and rearrangement (For an in-depth derivation I recommend [this](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/) blog post. [OpenAI](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) also has a great write-up about this, including a [proof](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html).) we can re-write the policy gradient as follows:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta}R(\\tau) = \\mathbb{E}_{\\pi_\\theta}\\left(\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\sum_{t^\\prime=t}^{T-1}\\gamma^{t^\\prime-t}R(s_{t^\\prime},a_{t^\\prime})\\right),\n",
    "$$\n",
    "so that actions are reinforced only by rewards following after taking them (I call this form temporally-adjusted rewards). I have outlined a bit more how this helps during training and how it fits into the more general reward formulation and variance reduction framework in [A. Alternative reward formulations and variance reduction](#appendix-a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufd3d9v204rJ"
   },
   "source": [
    "# Let's start by creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7eoNlww04rK"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# additional imports for saving and loading a trained policy\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "\n",
    "env = gym.make('CartPole-v1', new_step_api=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The notebook has been tested with the gym version 0.25.2**\n",
    "\n",
    "As mentioned above the state space in this environment is composed of four continuous values corresponding to the\n",
    "position and velocity of the cart, and the angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
    "\n",
    "Notice that in a real, physical environment the state of the cartpole problem would be much more complicated - it would include things like temperature, wind, friction between joints etc. In principle all of these could be measured and included in the observation vector, but it would be impossible to extract all the information about the state into an observation vector. In problems with partial observability (e.g. multiplayer games with imperfect information) the observations available to any one player are naturally a limited representation of the game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDBWtP5O04rL"
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the action space is discrete and corresponds to the possible actions (left, right) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2zNUxaz04rL"
   },
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ximNLG6_04rL"
   },
   "source": [
    "Now that we know that the dimensions of the observation and action space we can design a policy that takes in observations and produces probabilities of actions. \n",
    "\n",
    "The parametrized policy $\\pi_\\theta$ can be any differentiable function where $\\theta$ are the parameters. As seen in the lecture, for this problem we can use a plain logistic regression function to parametrize probabilities of moving left and right. Additionally, we can manually derive the gradients for the policy gradient update rule.\n",
    "\n",
    "More complex problems can use a neural network where $\\theta$ represents the learnable weights of the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitc Policy Function\n",
    "\n",
    "A simple function to implement the policy is the logistic function. You may remeber it from previous machine learning courses.\n",
    "\n",
    "The policy function is defined as $f(s) = 1 / (1+e^{-\\theta^{T}_{a} s})$,\n",
    "\n",
    "Where $\\theta$ are the function parameters, $x$ is the state (sometimes called observations), and $f(s)$ will correspond to the probablility of taking action $a$, accordint to policy $\\pi_{\\theta}$.\n",
    "\n",
    "In this environment, there are two actions $a_0, a_1$, hence,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(s) &= \\pi_{\\theta}(a=0 | s) = 1 / (1+e^{-\\theta^{T}_{0} s})\\\\\n",
    "f(s) &= \\pi_{\\theta}(a=1 | s) = 1 / (1+e^{-\\theta^{T}_{1} s})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can simplify the implementation given that $\\pi_{\\theta}(a=1 | s) = 1- \\pi_{\\theta}(a=0 | s)$, as follows, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi_{\\theta}(a=0 | s) &= 1 / (1+e^{-\\theta^{T}_{0} s})\\\\\n",
    "\\pi_{\\theta}(a=1 | s) &= 1 / (1+e^{\\theta^{T}_{0} s})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This means, that we only need to estimate one set of parameters $\\theta^{T}_{0}$\n",
    "\n",
    "As derived above, the policy gradient corresponds to:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta\\mathbb{E}_{\\pi_\\theta}R(\\tau) = \\mathbb{E}_{\\pi_\\theta}\\left(\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\sum_{t^\\prime=t}^{T-1}\\gamma^{t^\\prime-t}R(s_{t^\\prime},a_{t^\\prime})\\right),\n",
    "$$\n",
    "\n",
    "Where the first component corresponds to the gradient of the lof policy function $\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)$. For the logistic function, this corresponds to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta\\log\\pi_\\theta(a = 0\\vert s) &= s - s \\pi_\\theta(\\theta^{T}_{0}s)\\\\\n",
    "\\nabla_\\theta\\log\\pi_\\theta(a = 1\\vert s) &= s \\pi_\\theta(\\theta^{T}_{0}s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And the second component $\\sum_{t^\\prime=t}^{T-1}\\gamma^{t^\\prime-t}R(s_{t^\\prime},a_{t^\\prime})$ corresponds to the **return (sum of discounted rewards)** observed during trajectory $\\tau$ from time $t$ following policy $\\pi_{\\theta}$.\n",
    "\n",
    "**Observation:** There are different implementations of policy gradients on binary action space problems, some were using the Logistic Policy while others used [Softmax](https://en.wikipedia.org/wiki/Softmax_function). It turns out that Softmax applied to  a binary action space is not exactly equivalent to a Logistic policy - it has more parameters (8 for Softmax and 4 for Logistic in the cartpole example). More on this is shown below [B. Overparametrisation of Softmax](#appendix-b).\n",
    "\n",
    "Let's write now a Python class that will act as our Logistic policy agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNbhcpLE04rL"
   },
   "outputs": [],
   "source": [
    "class LogisticPolicy:\n",
    "\n",
    "    def __init__(self, θ, α, γ):\n",
    "        # Initialize paramters θ, learning rate α and discount factor γ\n",
    "        self.θ = θ\n",
    "        self.α = α\n",
    "        self.γ = γ\n",
    "\n",
    "    def logistic(self, y):\n",
    "        # definition of logistic function\n",
    "        \"\"\"TODO: Write the function that returns the value of the logistic function\"\"\"\n",
    "        #### ANSWER STARTS HERE ####\n",
    "        return 1/(1 + np.exp(-y))\n",
    "        #### ANSWER STOPS HERE ####\n",
    "\n",
    "    def probs(self, x):\n",
    "        # returns probabilities of two actions\n",
    "        \"\"\"TODO: Write the function that receives the states (observations) and returns the selected action (0 or 1) based on their \n",
    "        probabilities (computed using self.probs(x)). The function returns the action, and the probability of the selected action \"\"\"\n",
    "        #### ANSWER STARTS HERE ####\n",
    "        y = x @ self.θ\n",
    "        prob0 = self.logistic(y)\n",
    "        prob1 = 1-prob0\n",
    "        #### ANSWER STOPS HERE ####\n",
    "        return np.array([prob0, prob1])\n",
    "\n",
    "    def act(self, x):\n",
    "        # sample an action in proportion to probabilities\n",
    "        \"\"\"TODO: Write the function that receives the states (observations) and returns the action 0 or 1, based on their \n",
    "        probabilities (self.probs(x)\"\"\"\n",
    "        #### ANSWER STARTS HERE ####\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "        #### ANSWER STOPS HERE ####\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        # calculate grad-log-probs\n",
    "        \"\"\"TODO: Write the function that computes the gradient of the log of the policy function (see expression above) \"\"\"\n",
    "        #### ANSWER STARTS HERE ####\n",
    "        y = x @ self.θ\n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot grads with future rewards for each action in episode\n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        # calculate temporally adjusted, discounted rewards\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.γ + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # calculate gradients for each action over all observations\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "        \n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "\n",
    "        # calculate temporaly adjusted, discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "\n",
    "        # gradients times rewards\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, discounted_rewards)\n",
    "\n",
    "        # gradient ascent on parameters\n",
    "        self.θ += self.α*dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X06NN_MF04rM"
   },
   "source": [
    "**Note:** Se see [C. Efficient calculation of temporally adjusted discounted rewards](#appendix-c) for alternative ways to optimize the temporally-adjusted discounted_rewards function for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AcGiQ2c04rM"
   },
   "source": [
    "Let's also write a utility function that will run through one full episode and record all observations, actions taken and rewards received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiemKSlm04rM"
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observations.append(observation)\n",
    "\n",
    "        action, prob = policy.act(observation)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = truncated or terminated \n",
    "\n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "\n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9yrZFp504rM"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Finally, we write a training loop that will train an agent on the problem by repeated rollouts of a policy that is updated after the end of every episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkW2-uyR04rM"
   },
   "outputs": [],
   "source": [
    "def evaluate(Policy, seed=None, DIRNAME = './video_eval', MAX_EVAL_EPISODES = 100):\n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    \n",
    "    eval_rewards = []\n",
    "    policy = Policy\n",
    "\n",
    "    env = RecordVideo(env, DIRNAME,  episode_trigger = lambda episode_number: True, new_step_api=True)\n",
    "    env.reset()\n",
    "    print(\"Evaluating policy behaviour...\",end=\"\\r\", flush=False)\n",
    "    for i in range(MAX_EVAL_EPISODES):\n",
    "        run_episode(env, policy, render=False)\n",
    "        total_reward, _, _, _, _ = run_episode(env, policy, render=False)           \n",
    "        # keep track of episode rewards\n",
    "        eval_rewards.append(total_reward)\n",
    "    env.env.close()\n",
    "    return eval_rewards\n",
    "\n",
    "\n",
    "def train(Policy, MAX_EPISODES=1000, seed=None, evaluate=False, MAX_EVAL_EPISODES = 100):\n",
    "\n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    episode_rewards = []\n",
    "    eval_rewards = []\n",
    "    policy = Policy\n",
    "\n",
    "    # train until MAX_EPISODES\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        # run a single episode\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, policy)\n",
    "\n",
    "        # keep track of episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # update policy\n",
    "        policy.update(rewards, observations, actions)\n",
    "        print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \",end=\"\\r\", flush=False)\n",
    "\n",
    "    # evaluation call after training is finished - evaluate last trained policy on 100 episodes\n",
    "    if evaluate:\n",
    "        env = RecordVideo(env, './video_train',  episode_trigger = lambda episode_number: True, new_step_api=True)\n",
    "        env.reset()\n",
    "        for _ in range(MAX_EVAL_EPISODES):\n",
    "            total_reward, _, _, _, _ = run_episode(env, policy, render=False)           \n",
    "            # keep track of episode rewards\n",
    "            eval_rewards.append(total_reward)\n",
    "        env.env.close()\n",
    "\n",
    "    return episode_rewards, policy, eval_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAcTYQ2c04rM"
   },
   "source": [
    "Now, let's define the hyperparameters and run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLIkL6-w04rN"
   },
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "GLOBAL_SEED = 0\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# Define Hyperparameters\n",
    "θ=np.random.rand(4)\n",
    "α=0.002\n",
    "γ=0.99\n",
    "\n",
    "\n",
    "# Initialize policy\n",
    "policy = LogisticPolicy(θ, α, γ)\n",
    "\n",
    "# Evaluate policy behaviour\n",
    "evaluate(policy,\n",
    "         seed=GLOBAL_SEED,\n",
    "         DIRNAME = './video_no_training',\n",
    "         MAX_EVAL_EPISODES = 10)\n",
    "\n",
    "\n",
    "# Train the policy\n",
    "episode_rewards, policy,_ = train(policy,\n",
    "                                MAX_EPISODES=800,\n",
    "                                seed=GLOBAL_SEED,\n",
    "                                evaluate=False,\n",
    "                                MAX_EVAL_EPISODES = 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iynZFf0b04rN"
   },
   "source": [
    "Let's inspect the results by plotting the reward against episode number during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuynmX8a04rN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(episode_rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp9HL5LC04rN"
   },
   "source": [
    "## Tasks\n",
    "\n",
    "1. **Plotting:** Write below the code to plot how many steps was the policy able to keep the pole balanced after training\n",
    "2. **Learning:** Is the agent able to learn a good policy? How stable is the performance?\n",
    "3. **Training length:** Would increasing the number of episodes to 1'200 lead to more stable performance?\n",
    "4. **Stability per episode:** Several approaches can be made to improve stability. For instance, we can increase the number of episodes per gradient update or introduce a learning rate scheduling. Update the code to try this approach.\n",
    "5. **Stability across episodes:** Run the training 10 times with different random seeds and estimate the mean performance over time (with standard errors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlVBY5nw04rN"
   },
   "outputs": [],
   "source": [
    "\"\"\"TODO: Write the function to plot the number of steps the trained policy is able to keep the pole balanced\"\"\"\n",
    "#### ANSWER STARTS HERE ####\n",
    "# run a few episodes using the trained policy (without learning) \n",
    "eval_rewards = evaluate(policy, DIRNAME = './video_after_training',\n",
    "                        seed=GLOBAL_SEED, MAX_EVAL_EPISODES = 50)\n",
    "\n",
    "# Plot Results per episode\n",
    "plt.figure(0)\n",
    "plt.plot(eval_rewards)\n",
    "plt.figure(1)\n",
    "plt.hist(eval_rewards, bins=20);\n",
    "#### ANSWER STOPS HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1hyrjc504rN"
   },
   "source": [
    "# Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfnMwvmr04rN"
   },
   "source": [
    "Additional material for those interested in diving deeper in the topic (these material will not be part of the exam)\n",
    "\n",
    "Lilian Weng's [A (Long) Peek into RL](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) and [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)\n",
    "\n",
    "[Daniel Takeshi's derivations of policy gradients](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/)\n",
    "\n",
    "[Sutton et. al. Policy Gradients in NIPS 1998](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDICES - Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwdobTgY04rO"
   },
   "source": [
    "# A. Overparametrisation of Softmax <a class=\"anchor\" id=\"appendix-a\"></a>\n",
    "\n",
    "In logistic regression we use the parametrized logistic function $f_\\theta(x)=\\dfrac{e^{\\theta\\cdot x}}{1+e^{\\theta\\cdot x}}$ where $\\theta, x$ are the $k$-dimensional weight and feature vectors respectively (we assume the bias term is incorporated within $\\theta$ and $x$). Because there are only two classes, we interpret the value of $f_\\theta(x)$ as $p(y=1\\vert \\theta,x)$, i.e. the probability of the first class. Then we get the probability of the second class for free since probabilities must sum to one.\n",
    "\n",
    "The parametrized softmax function defined as $\\sigma_\\theta(x)_i=\\dfrac{e^{\\theta_i\\cdot x}}{\\sum_{j=1}^{m}e^{\\theta_j\\cdot x}}$ for $i=1,\\dots,m$ is a generalization of the logistic function to $m$ output classes and extends logistic regression to multi-class problems. We interpret the value of $\\sigma_\\theta(x)_i$ as $p(y=i\\vert \\theta_i, x)$ for $i=1,\\dots,m$. Note that with softmax there is a separate weight vector $\\theta_i$ for each of the classes.\n",
    "\n",
    "There is a subtle difference in going from the logistic function to the softmax function that is rarely explained but becomes apparent when you compare using the softmax function instead of the logistic function for a two-class problem. Essentially, the logistic function only maintains a parameter vector $\\theta$ of length $k$ for estimating the first class probability while the softmax function maintains a separate parameter vector for each of the classes, thus in the two-class case a softmax formulation would have $2k$ parameters - twice as many as in logistic regression. This means that the softmax formulation results in redundant parameters - this is called overparametrization.\n",
    "\n",
    "Let's write this out in detail. The class probabilities for an m-class problem are given by\n",
    "\n",
    "$$\n",
    "p(y=i\\vert \\theta_i,x) = \\dfrac{e^{\\theta_i\\cdot x}}{\\sum_{j=1}^{m}e^{\\theta_j\\cdot x}} \\text{ ,for $i=1,\\dots,m$}.  \n",
    "$$\n",
    "\n",
    "Let $\\phi$ be some fixed vector and note that if we replace all $\\theta_i$ by $\\theta_i-\\phi$, the probabilities are unchanged:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{e^{(\\theta_i-\\phi)\\cdot x}}{\\sum_{j=1}^{m}e^{(\\theta_j-\\phi)\\cdot x}} &= \\\\\n",
    "&= \\dfrac{e^{-\\phi\\cdot x}e^{\\theta_i\\cdot x}}{e^{-\\phi\\cdot x}\\sum_{j=1}^{m}e^{\\theta_j\\cdot x}} \\\\\n",
    "&= p(y=i\\vert \\theta_i,x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In particular, we can pick $\\phi=\\theta_0$ thus setting the first parameter vector to be identically zero in effect eliminating $k$ redundant parameters from calculations. Doing so in the $2$-class problem we would recover ordinary logistic regression with $k$ parameters, i.e. setting $\\theta_0\\to 0$ and $\\theta_1\\to\\theta_1-\\theta_0=:\\theta$ would give $p_0=\\dfrac{1}{1+e^{\\theta\\cdot x}}$ and $p_1=\\dfrac{e^{\\theta\\cdot x}}{1+e^{\\theta\\cdot x}}$ as in vanilla logistic regression.\n",
    "\n",
    "In our cartpole example because we used a vanilla logistic policy we ended up with four trainable weights, one for each of the observations. However, had we gone for a softmax policy with two classes we would have eight trainable weights instead.\n",
    "\n",
    "In an $m$-class problem, the number of weights maintained by softmax is $m\\times k$ but $k$ of these can be eliminated as shown. In practice, however, it is less hassle to keep the softmax formulation as is since enforcing some parameters to be zero would lead to less clean code and extra difficulties when calculating gradients.\n",
    "\n",
    "For more discussion on the softmax function look [here](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) and for more details concerning the overparametrization of softmax look [here](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw42E1hM04rO"
   },
   "source": [
    "# B. Efficient calculation of temporally adjusted discounted rewards <a class=\"anchor\" id=\"appendix-b\"></a>\n",
    "\n",
    "Looking at the training code, a potential inefficiency is in the function that calculates the discounted rewards for each step in an episode. Since it is a for loop in Python it is worth investigating whether it can be sped up by vectorizing.\n",
    "\n",
    "First, let's time the original function with some random vectors of fixed size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_naAdxoy04rO"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for i in reversed(range(0, len(rewards))):\n",
    "        cumulative_rewards = cumulative_rewards * gamma + rewards[i]\n",
    "        discounted_rewards[i] = cumulative_rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXfgGKcG04rO"
   },
   "outputs": [],
   "source": [
    "%timeit discount_rewards(np.random.rand(100), 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUMsXN-L04rO"
   },
   "source": [
    "## First attempt: vectorizing\n",
    "The first thing when you see a for loop in numerical Python code is trying to see if you can vectorize it for performance. If we let $\\mathbf{r,\\hat{r}}$ be the vectors containing the original and discounted rewards respectively for each step in the episode then we can write the above for loop as a matrix equation $\\mathbf{\\hat{r}}=\\mathbf{\\Gamma r}$, where\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Gamma} =  \\begin{bmatrix}1 & \\gamma & \\gamma^2 & \\cdots & \\gamma^{n-1} \\\\\n",
    "0 & 1 & \\gamma & \\cdots & \\gamma^{n-2} \\\\\n",
    "\\vdots & & \\ddots & \\\\\n",
    "0 & 0 & \\cdots & 1 & \\gamma \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This type of matrix is known as a [Toeplitz matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix) (research hack: I did not know the name of this type of matrix but I thought it must have one given its special properties, so I typed in an example matrix into [Wolfram Alpha](https://www.wolframalpha.com/input/?i=%5B%5B1,a,a%5E2%5D,%5B0,1,a%5D,%5B0,0,1%5D%5D) which kindly provided me with a name). Scipy has a function ```scipy.linalg.toeplitz``` for constructing such matrices, so let's try to use it to rewrite our code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfwIdMsI04rO"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "\n",
    "def toeplitz_discount_rewards(rewards, gamma):\n",
    "    n = len(rewards)\n",
    "    c = np.zeros_like(rewards)\n",
    "    c[0] = 1\n",
    "\n",
    "    r = np.array([gamma**i for i in range(n)])\n",
    "    matrix = sp.linalg.toeplitz(c, r)\n",
    "    discounted_rewards = matrix @ rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFTu9aRl04rP"
   },
   "source": [
    "And time this for comparison with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1bFXzf804rP"
   },
   "outputs": [],
   "source": [
    "%timeit toeplitz_discount_rewards(np.random.rand(100), 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTTI4Ohj04rP"
   },
   "source": [
    "So in fact this is much worse than the original implementation. This is not a big surprise since there is a lot of overhead in building the matrix and then doing the matrix-vector calculation, so the single pass over the reward vector in the original loop seems pretty performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v54izpMq04rP"
   },
   "source": [
    "## Second attempt: OpenAI hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22_WHwyd04rQ"
   },
   "source": [
    "While reading the source code of OpenAI's excellent RL learning library [spinning up](https://spinningup.openai.com/en/latest/) I came across [this curious implementation of discounted rewards](https://github.com/openai/spinningup/blob/fc75b23d539ad1f511e537df9abf21a7aa329706/spinup/algos/vpg/core.py#L45):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjBBh1Xt04rQ"
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def magic_discount_rewards(rewards, gamma):\n",
    "    return sp.signal.lfilter([1], [1, float(-gamma)], rewards[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0gRbASC04rQ"
   },
   "source": [
    "This is using the Scipy signal processing library, using a digital filter on a 1D data sequence ([see the docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYIj3gEy04rQ"
   },
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCC6vL3D04rR"
   },
   "outputs": [],
   "source": [
    "%timeit magic_discount_rewards(np.random.rand(100), 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vouqauR04rR"
   },
   "source": [
    "This is two to three times faster than the original implementation! Well worth using for problems where this calculation is frequent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
