{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Learning on the CartPole Environment\n",
    "\n",
    "In this practice we will policy learning algorithms to the CartPole-v1 environment from the [OpenAI Gym](https://gym.openai.com/). In this environment, the task is to balance the pole that is attached to the cart, by moving the cart to either side. The reward gets incremented for each step (for up to 200 steps) where the pole is not exceeding a set angle and the cart is not touching the sides of the line.\n",
    "\n",
    "![cartpole](https://github.com/pytorch/tutorials/blob/main/_static/img/cartpole.gif?raw=true)\n",
    "\n",
    "The environment provides four parameters that represent the state of the environment:\n",
    "Position and velocity of the cart and angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
    "\n",
    "In this notebook, we will implement Q-learning for this environment. As you will notice, the state space of Cartpole uses continues values, hence we will need to discretize, in order to apply the tabular version of Q-Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2MqEqos04rH"
   },
   "source": [
    "# Policy gradients\n",
    "\n",
    "Policy gradients is a family of algorithms for solving reinforcement learning problems by directly optimizing the policy in policy space. This is in stark contrast to value based approaches (such as Q-learning used previous practices or in [Learning Atari games by DeepMind](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning). This practice implements the Vanilla Policy Gradient (VPG aka REINFORCE).\n",
    "\n",
    "Policy gradients have several appealing properties, for one they produce stochastic policies (by learning a probability distribution over actions given observations) whereas value based approaches are deterministic as they will typically choose actions greedily with respect to the value function being learned which can lead to under-exploration (one needs to introduce exploration strategies such as $\\epsilon$-greedy explicitly to get around this). Another advantage of policy gradients is their ability to tackle continuous action spaces without discretisation which is necessary for value based methods. \n",
    "\n",
    "One of the biggest disadvantages of policy gradients is their high variance estimates of the gradient updates. This leads to very noisy gradient estimates and can de-stabilize the learning process. Significant efforts in reinforcement learning research with policy gradients in the past few years has been about trying to reduce the variance of these gradient updates to improve the trainability of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufd3d9v204rJ"
   },
   "source": [
    "# Let's start by creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7eoNlww04rK"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# additional imports for saving and loading a trained policy\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "\n",
    "env = gym.make('CartPole-v1', new_step_api=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above the state space in this environment is composed of four continuous values corresponding to the\n",
    "position and velocity of the cart, and the angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
    "\n",
    "Notice that in a real, physical environment the state of the cartpole problem would be much more complicated - it would include things like temperature, wind, friction between joints etc. In principle all of these could be measured and included in the observation vector, but it would be impossible to extract all the information about the state into an observation vector. In problems with partial observability (e.g. multiplayer games with imperfect information) the observations available to any one player are naturally a limited representation of the game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDBWtP5O04rL"
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the action space is discrete and corresponds to the possible actions (left, right) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2zNUxaz04rL"
   },
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ximNLG6_04rL"
   },
   "source": [
    "Now that we know that the dimensions of the observation and action space we can design a policy that takes in observations and produces probabilities of actions. \n",
    "\n",
    "The parametrized policy $\\pi_\\theta$ can be any differentiable function where $\\theta$ are the parameters. As seen in the lecture, for this problem we can use a plain logistic regression function to parametrize probabilities of moving left and right. Additionally, we can manually derive the gradients for the policy gradient update rule.\n",
    "\n",
    "More complex problems can use a neural network where $\\theta$ represents the learnable weights of the network. We will cover this case later in the course. \n",
    "\n",
    "**Observation:** There are different implementations of policy gradients on binary action space problems, some were using the Logistic Policy while others used [Softmax](https://en.wikipedia.org/wiki/Softmax_function). It turns out that Softmax applied to  a binary action space is not exactly equivalent to a Logistic policy - it has more parameters (8 for Softmax and 4 for Logistic in the cartpole example). More on this is shown below [B. Overparametrisation of Softmax](#appendix-b).\n",
    "\n",
    "Let's write now a Python class that will act as our Logistic policy agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNbhcpLE04rL"
   },
   "outputs": [],
   "source": [
    "class LogisticPolicy:\n",
    "\n",
    "    def __init__(self, θ, α, γ):\n",
    "        # Initialize paramters θ, learning rate α and discount factor γ\n",
    "\n",
    "        self.θ = θ\n",
    "        self.α = α\n",
    "        self.γ = γ\n",
    "\n",
    "    def logistic(self, y):\n",
    "        # definition of logistic function\n",
    "\n",
    "        return 1/(1 + np.exp(-y))\n",
    "\n",
    "    def probs(self, x):\n",
    "        # returns probabilities of two actions\n",
    "\n",
    "        y = x @ self.θ\n",
    "        prob0 = self.logistic(y)\n",
    "\n",
    "        return np.array([prob0, 1-prob0])\n",
    "\n",
    "    def act(self, x):\n",
    "        # sample an action in proportion to probabilities\n",
    "\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        # calculate grad-log-probs\n",
    "\n",
    "        y = x @ self.θ\n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot grads with future rewards for each action in episode\n",
    "\n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        # calculate temporally adjusted, discounted rewards\n",
    "\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.γ + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # calculate gradients for each action over all observations\n",
    "        \n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "        \n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "\n",
    "        # calculate temporaly adjusted, discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "\n",
    "        # gradients times rewards\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, discounted_rewards)\n",
    "\n",
    "        # gradient ascent on parameters\n",
    "        self.θ += self.α*dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X06NN_MF04rM"
   },
   "source": [
    "**Tangent:** during the writing of the temporally-adjusted discounted_rewards function I started thinking if one can optimize the for loop for better performance. For my attempts see [C. Efficient calculation of temporally adjusted discounted rewards](#appendix-c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AcGiQ2c04rM"
   },
   "source": [
    "Let's also write a utility function that will run through one full episode and record all observations, actions taken and rewards received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiemKSlm04rM"
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observations.append(observation)\n",
    "\n",
    "        action, prob = policy.act(observation)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = truncated or terminated \n",
    "\n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "\n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9yrZFp504rM"
   },
   "source": [
    "Finally, we write a training loop that will train an agent on the problem by repeated rollouts of a policy that is updated after the end of every episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkW2-uyR04rM"
   },
   "outputs": [],
   "source": [
    "def evaluate(θ, α, γ, Policy, seed=None):\n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    episode_rewards = []\n",
    "    policy = Policy(θ, α, γ)\n",
    "\n",
    "    env = RecordVideo(env, './video_eval',  episode_trigger = lambda episode_number: True, new_step_api=True)\n",
    "    env.reset()\n",
    "    for _ in range(100):\n",
    "        run_episode(env, policy, render=False)\n",
    "    env.env.close()\n",
    "\n",
    "\n",
    "def train(θ, α, γ, Policy, MAX_EPISODES=1000, seed=None, evaluate=False):\n",
    "\n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    episode_rewards = []\n",
    "    policy = Policy(θ, α, γ)\n",
    "\n",
    "    # train until MAX_EPISODES\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        # run a single episode\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, policy)\n",
    "\n",
    "        # keep track of episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # update policy\n",
    "        policy.update(rewards, observations, actions)\n",
    "        print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \",end=\"\\r\", flush=False)\n",
    "\n",
    "    # evaluation call after training is finished - evaluate last trained policy on 100 episodes\n",
    "    if evaluate:\n",
    "        env = RecordVideo(env, './video_train',  episode_trigger = lambda episode_number: True, new_step_api=True)\n",
    "        env.reset()\n",
    "        for _ in range(100):\n",
    "            run_episode(env, policy, render=False)\n",
    "        env.env.close()\n",
    "\n",
    "    return episode_rewards, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAcTYQ2c04rM"
   },
   "source": [
    "Now, let's run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLIkL6-w04rN"
   },
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "GLOBAL_SEED = 0\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "evaluate(θ=np.random.rand(4),\n",
    "  α=0.002,\n",
    "  γ=0.99,\n",
    "  Policy=LogisticPolicy,\n",
    "  seed=GLOBAL_SEED)\n",
    "\n",
    "episode_rewards, policy = train(θ=np.random.rand(4),\n",
    "                                α=0.002,\n",
    "                                γ=0.99,\n",
    "                                Policy=LogisticPolicy,\n",
    "                                MAX_EPISODES=800,\n",
    "                                seed=GLOBAL_SEED,\n",
    "                                evaluate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iynZFf0b04rN"
   },
   "source": [
    "Let's inspect the results by plotting the reward against episode number during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuynmX8a04rN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(episode_rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wp9HL5LC04rN"
   },
   "source": [
    "## Tasks\n",
    "\n",
    "1. **Plotting** Write below the code to plot how many steps was the poilicy able to keep the pole balanced after training\n",
    "2. **Learning** Is the agent able to learn a good policy? How stable is the performance?\n",
    "3. **Training length** Would increasing the number of episodes to 1'200 lead to more stable performance?\n",
    "4. **Stability per episode** Several approaches can be made to improve stability. For instance, we can increase the number of episodes per gradient update or introduce a learning rate scheduling\n",
    "5. **Stability across episodes**Run the training 10 times with different random seeds and estimate the mean performance over time (with standard errors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlVBY5nw04rN"
   },
   "outputs": [],
   "source": [
    "# Plot Results per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1hyrjc504rN"
   },
   "source": [
    "# Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfnMwvmr04rN"
   },
   "source": [
    "Additional material for those interested in diving deeper in the topic (these material will not be part of the exam)\n",
    "\n",
    "Lilian Weng's [A (Long) Peek into RL](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) and [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)\n",
    "\n",
    "[Daniel Takeshi's derivations of policy gradients](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/)\n",
    "\n",
    "[Sutton et. al. Policy Gradients in NIPS 1998](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDICES - Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwdobTgY04rO"
   },
   "source": [
    "# A. Overparametrisation of Softmax <a class=\"anchor\" id=\"appendix-a\"></a>\n",
    "\n",
    "In logistic regression we use the parametrized logistic function $f_\\theta(x)=\\dfrac{e^{\\theta\\cdot x}}{1+e^{\\theta\\cdot x}}$ where $\\theta, x$ are the $k$-dimensional weight and feature vectors respectively (we assume the bias term is incorporated within $\\theta$ and $x$). Because there are only two classes, we interpret the value of $f_\\theta(x)$ as $p(y=1\\vert \\theta,x)$, i.e. the probability of the first class. Then we get the probability of the second class for free since probabilities must sum to one.\n",
    "\n",
    "The parametrized softmax function defined as $\\sigma_\\theta(x)_i=\\dfrac{e^{\\theta_i\\cdot x}}{\\sum_{j=1}^{m}e^{\\theta_j\\cdot x}}$ for $i=1,\\dots,m$ is a generalization of the logistic function to $m$ output classes and extends logistic regression to multi-class problems. We interpret the value of $\\sigma_\\theta(x)_i$ as $p(y=i\\vert \\theta_i, x)$ for $i=1,\\dots,m$. Note that with softmax there is a separate weight vector $\\theta_i$ for each of the classes.\n",
    "\n",
    "There is a subtle difference in going from the logistic function to the softmax function that is rarely explained but becomes apparent when you compare using the softmax function instead of the logistic function for a two-class problem. Essentially, the logistic function only maintains a parameter vector $\\theta$ of length $k$ for estimating the first class probability while the softmax function maintains a separate parameter vector for each of the classes, thus in the two-class case a softmax formulation would have $2k$ parameters - twice as many as in logistic regression. This means that the softmax formulation results in redundant parameters - this is called overparametrization.\n",
    "\n",
    "Let's write this out in detail. The class probabilities for an m-class problem are given by\n",
    "\n",
    "$$\n",
    "p(y=i\\vert \\theta_i,x) = \\dfrac{e^{\\theta_i\\cdot x}}{\\sum_{j=1}^{m}e^{\\theta_j\\cdot x}} \\text{ ,for $i=1,\\dots,m$}.  \n",
    "$$\n",
    "\n",
    "Let $\\phi$ be some fixed vector and note that if we replace all $\\theta_i$ by $\\theta_i-\\phi$, the probabilities are unchanged:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{e^{(\\theta_i-\\phi)\\cdot x}}{\\sum_{j=1}^{m}e^{(\\theta_j-\\phi)\\cdot x}} &= \\\\\n",
    "&= \\dfrac{e^{-\\phi\\cdot x}e^{\\theta_i\\cdot x}}{e^{-\\phi\\cdot x}\\sum_{j=1}^{m}e^{\\theta_j\\cdot x}} \\\\\n",
    "&= p(y=i\\vert \\theta_i,x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In particular, we can pick $\\phi=\\theta_0$ thus setting the first parameter vector to be identically zero in effect eliminating $k$ redundant parameters from calculations. Doing so in the $2$-class problem we would recover ordinary logistic regression with $k$ parameters, i.e. setting $\\theta_0\\to 0$ and $\\theta_1\\to\\theta_1-\\theta_0=:\\theta$ would give $p_0=\\dfrac{1}{1+e^{\\theta\\cdot x}}$ and $p_1=\\dfrac{e^{\\theta\\cdot x}}{1+e^{\\theta\\cdot x}}$ as in vanilla logistic regression.\n",
    "\n",
    "In our cartpole example because we used a vanilla logistic policy we ended up with four trainable weights, one for each of the observations. However, had we gone for a softmax policy with two classes we would have eight trainable weights instead.\n",
    "\n",
    "In an $m$-class problem, the number of weights maintained by softmax is $m\\times k$ but $k$ of these can be eliminated as shown. In practice, however, it is less hassle to keep the softmax formulation as is since enforcing some parameters to be zero would lead to less clean code and extra difficulties when calculating gradients.\n",
    "\n",
    "For more discussion on the softmax function look [here](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) and for more details concerning the overparametrization of softmax look [here](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw42E1hM04rO"
   },
   "source": [
    "# B. Efficient calculation of temporally adjusted discounted rewards <a class=\"anchor\" id=\"appendix-b\"></a>\n",
    "\n",
    "Looking at the training code, a potential inefficiency is in the function that calculates the discounted rewards for each step in an episode. Since it is a for loop in Python it is worth investigating whether it can be sped up by vectorizing.\n",
    "\n",
    "First, let's time the original function with some random vectors of fixed size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_naAdxoy04rO"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for i in reversed(range(0, len(rewards))):\n",
    "        cumulative_rewards = cumulative_rewards * gamma + rewards[i]\n",
    "        discounted_rewards[i] = cumulative_rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXfgGKcG04rO"
   },
   "outputs": [],
   "source": [
    "%timeit discount_rewards(np.random.rand(100), 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUMsXN-L04rO"
   },
   "source": [
    "## First attempt: vectorizing\n",
    "The first thing when you see a for loop in numerical Python code is trying to see if you can vectorize it for performance. If we let $\\mathbf{r,\\hat{r}}$ be the vectors containing the original and discounted rewards respectively for each step in the episode then we can write the above for loop as a matrix equation $\\mathbf{\\hat{r}}=\\mathbf{\\Gamma r}$, where\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Gamma} =  \\begin{bmatrix}1 & \\gamma & \\gamma^2 & \\cdots & \\gamma^{n-1} \\\\\n",
    "0 & 1 & \\gamma & \\cdots & \\gamma^{n-2} \\\\\n",
    "\\vdots & & \\ddots & \\\\\n",
    "0 & 0 & \\cdots & 1 & \\gamma \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This type of matrix is known as a [Toeplitz matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix) (research hack: I did not know the name of this type of matrix but I thought it must have one given its special properties, so I typed in an example matrix into [Wolfram Alpha](https://www.wolframalpha.com/input/?i=%5B%5B1,a,a%5E2%5D,%5B0,1,a%5D,%5B0,0,1%5D%5D) which kindly provided me with a name). Scipy has a function ```scipy.linalg.toeplitz``` for constructing such matrices, so let's try to use it to rewrite our code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfwIdMsI04rO"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "\n",
    "def toeplitz_discount_rewards(rewards, gamma):\n",
    "    n = len(rewards)\n",
    "    c = np.zeros_like(rewards)\n",
    "    c[0] = 1\n",
    "\n",
    "    r = np.array([gamma**i for i in range(n)])\n",
    "    matrix = sp.linalg.toeplitz(c, r)\n",
    "    discounted_rewards = matrix @ rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFTu9aRl04rP"
   },
   "source": [
    "And time this for comparison with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1bFXzf804rP"
   },
   "outputs": [],
   "source": [
    "%timeit toeplitz_discount_rewards(np.random.rand(100), 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTTI4Ohj04rP"
   },
   "source": [
    "So in fact this is much worse than the original implementation. This is not a big surprise since there is a lot of overhead in building the matrix and then doing the matrix-vector calculation, so the single pass over the reward vector in the original loop seems pretty performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v54izpMq04rP"
   },
   "source": [
    "## Second attempt: OpenAI hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22_WHwyd04rQ"
   },
   "source": [
    "While reading the source code of OpenAI's excellent RL learning library [spinning up](https://spinningup.openai.com/en/latest/) I came across [this curious implementation of discounted rewards](https://github.com/openai/spinningup/blob/fc75b23d539ad1f511e537df9abf21a7aa329706/spinup/algos/vpg/core.py#L45):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjBBh1Xt04rQ"
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def magic_discount_rewards(rewards, gamma):\n",
    "    return sp.signal.lfilter([1], [1, float(-gamma)], rewards[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0gRbASC04rQ"
   },
   "source": [
    "This is using the Scipy signal processing library, using a digital filter on a 1D data sequence ([see the docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYIj3gEy04rQ"
   },
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCC6vL3D04rR"
   },
   "outputs": [],
   "source": [
    "%timeit magic_discount_rewards(np.random.rand(100), 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vouqauR04rR"
   },
   "source": [
    "This is two to three times faster than the original implementation! Well worth using for problems where this calculation is frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgWKtbTw04rR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
