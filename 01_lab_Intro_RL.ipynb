{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjuBuBeVi6pE"
   },
   "source": [
    "\n",
    "# Code-Driven Introduction to Reinforcement Learning\n",
    "\n",
    "Welcome to the first practice of our course. Today's practice is inspired by an exercise from the book [Reinforcement Learning](https://rl-book.com/?utm_source=winder&utm_medium=notebook&utm_campaign=rl), by Prof. Dr. Phil Winder. \n",
    "\n",
    "In this notebook you will be investigating the fundamentals of reinforcement learning (RL).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This practice was developed as a jupyter notebook. You can use a local installation in your machine or an online host like [Binder](https://mybinder.org/), [Google's colaboratory](https://colab.research.google.com/) or similar. No significant computer power is needed to run this practice.\n",
    "\n",
    "We expect that you are familiar with a Python and Jupyter notebooks.\n",
    "\n",
    "\n",
    "### Expectations\n",
    "\n",
    "The practice uses a very simple and intuitive example that we saw already in the lecture. This makes it easier to understand, since that is the main goal.\n",
    "\n",
    "More comples examples will be covered later in the course. You can see some [industrial examples here](https://rl-book.com/applications/?utm_source=winder&utm_medium=notebook&utm_campaign=rl).\n",
    "\n",
    "> We do not expect you to understand every detail presented in this notebook.\n",
    "> Today we focus on learning the rationale behind Reinforcement Learning.\n",
    "\n",
    "## The Internals of Reinforcement Learning\n",
    "\n",
    "Before we dive in, let's refresh some certain terms/key components:\n",
    "\n",
    "- **An agent**: An application that is able to observe state and suggest an action. It also receives feedback to let it know whether the action was good, or not.\n",
    "- **An environment**: This is the place where the agent acts within. It accepts an action, which alters its internal state, and produces a new observation of that state.\n",
    "\n",
    "In nearly all of the examples we see in literature, these two entities are implemented independently. The agent is a RL algorithm and the environment is either real life or a simulation.\n",
    "\n",
    "\n",
    "The agent and the environment interact through an interface. You have some control over what goes into that interface and a large amount of effort is typically spent improving the quality of the data that flows through it. You need representations of:\n",
    "\n",
    "- **State**: This is the observation of the environment. You often get to choose what to \"show\" to the agent. There is a compromise between simplifying the state to speed up learning and preventing overfitting, but often it pays to include as much as you can.\n",
    "- **Action**: Your agent must suggest an action. This mutates the environment in some way. So called \"options\" or \"null-actions\" allow you to do nothing, if that's what you want to do.\n",
    "- **Reward**: You use the reward to fine-tune your action choices.\n",
    "\n",
    "### Creating a \"GridWorld\" Environment\n",
    "\n",
    "Let's start creating your first environment: a simulation of a simple grid-based \"world\". Many real-life implementations begin with a simulation of the real world, because it's much easier to iterate and improve your design with a software stub of real-life.\n",
    "\n",
    "The goal of this environment is to define a \"world\" in which a \"robot\" can move. The so-called-world is actually a series of cells inside a 2-dimensional box. The agent can move north, east, south, or west which moves the robot between the cells. The goal of the environment is to reach a goal. There is a reward of -1 for every step, to promote reaching the goal as fast as possible.\n",
    "\n",
    "#### Imports and Definitions\n",
    "\n",
    "First let's import a few libraries (to enable the autocompletion in later cells) and define a few important definitions. The first is the defacto definition of a \"point\" object, with x and y coordinates and the second is a direction enumeration. These are use to define the position of the agent in the environment and the direction of movement for an action, respectively. Note that we assume that east moves in a positive x direction and north moves in a positive y direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZQ38NOIkwCi0"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from enum import Enum\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "class Direction(Enum):\n",
    "  NORTH = \"⬆\"\n",
    "  EAST = \"⮕\"\n",
    "  SOUTH = \"⬇\"\n",
    "  WEST = \"⬅\"\n",
    "\n",
    "  @classmethod\n",
    "  def values(self):\n",
    "    return [v for v in self]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV_vGVIaiZPz"
   },
   "source": [
    "#### The Environment Class\n",
    "\n",
    "Now, we create a Python class that represents the environment. The first function in the class is the initialisation function in which we can specify the width and height of the environment.\n",
    "\n",
    "After that, we define a helper parameter which encodes the possible actions and then we reset the state of the environment with a `reset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k64kfJi3jBaq"
   },
   "outputs": [],
   "source": [
    "class SimpleGridWorld(object):\n",
    "  def __init__(self, width: int = 5, height: int = 5, debug: bool = False):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.debug = debug\n",
    "    self.action_space = [d for d in Direction]\n",
    "    self.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHXiX7RAjEdf"
   },
   "source": [
    "#### The Reset Function\n",
    "\n",
    "Many environments have an implicit \"reset\", whereby the environment's state is moved away from the goal state. In this implementation, reset takes the state of the environment back to the `(0, 0)` position, but this isn't strictly necessary. Many real-life environments reset randomly or have no reset at all.\n",
    "\n",
    "We also set the goal, which is located in the south-eastern corner of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TkPQCnrRjqIO"
   },
   "outputs": [],
   "source": [
    "class SimpleGridWorld(SimpleGridWorld):\n",
    "  def reset(self):\n",
    "    self.cur_pos = Point(x=0, y=(self.height - 1))\n",
    "    self.goal = Point(x=(self.width - 1), y=0)\n",
    "    # If debug, print state\n",
    "    if self.debug:\n",
    "      print(self)\n",
    "    return self.cur_pos, 0, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f70YnYwGjtwV"
   },
   "source": [
    "#### Taking a Step\n",
    "\n",
    "Recall three of the key components we defined earlier: the state, the action, and the reward. The environment's step function accepts an action, then produces a new state and reward.\n",
    "\n",
    "The large amount of code is a consequence of the direction implementation. You can refactor this to use fewer lines of code with some clever indexing. However, we'll keep  this level of verbosity to make it easier to see what is going on. In essence, every direction moves the current position by one square. You can see the code incrementing or decrementing the x or y coordinates.\n",
    "\n",
    "The second part of the function is testing to see if the agent is at the goal. If it is, then it signals that it is at a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7rCJNuVnjtA2"
   },
   "outputs": [],
   "source": [
    "class SimpleGridWorld(SimpleGridWorld):\n",
    "  def step(self, action: Direction):\n",
    "    # Depending on the action, mutate the environment state\n",
    "    if action == Direction.NORTH:\n",
    "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
    "    elif action == Direction.EAST:\n",
    "      self.cur_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
    "    elif action == Direction.SOUTH:\n",
    "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
    "    elif action == Direction.WEST:\n",
    "      self.cur_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
    "    # Check if out of bounds\n",
    "    if self.cur_pos.x >= self.width:\n",
    "      self.cur_pos = Point(self.width - 1, self.cur_pos.y)\n",
    "    if self.cur_pos.y >= self.height:\n",
    "      self.cur_pos = Point(self.cur_pos.x, self.height - 1)\n",
    "    if self.cur_pos.x < 0:\n",
    "      self.cur_pos = Point(0, self.cur_pos.y)\n",
    "    if self.cur_pos.y < 0:\n",
    "      self.cur_pos = Point(self.cur_pos.x, 0)\n",
    "\n",
    "    # If at goal, terminate\n",
    "    is_terminal = self.cur_pos == self.goal\n",
    "\n",
    "    # Constant -1 reward to promote speed-to-goal\n",
    "    reward = -1\n",
    "\n",
    "    # If debug, print state\n",
    "    if self.debug:\n",
    "      print(self)\n",
    "\n",
    "    return self.cur_pos, reward, is_terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDHb8l_-lJKn"
   },
   "source": [
    "#### Visualisation\n",
    "\n",
    "And finally, like all of data science, it is vitally important that you are able to visualise the behaviour and performance of your agent. The first step in this process is being able to visualise the agent within the environment. The next function does this by printing a textual grid, with an `x` at the agent's location, a `o` at the goal, an `@` if the agent is on top of the goal, and a `_` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HGOg2vh_sNEA"
   },
   "outputs": [],
   "source": [
    "class SimpleGridWorld(SimpleGridWorld):\n",
    "  def __repr__(self):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(self.height)):\n",
    "      for x in range(self.width):\n",
    "        if self.goal.x == x and self.goal.y == y:\n",
    "          if self.cur_pos.x == x and self.cur_pos.y == y:\n",
    "            res += \"@\"\n",
    "          else:\n",
    "            res += \"o\"\n",
    "          continue\n",
    "        if self.cur_pos.x == x and self.cur_pos.y == y:\n",
    "          res += \"x\"\n",
    "        else:\n",
    "          res += \"_\"\n",
    "      res += \"\\n\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w3o6SH6lykl"
   },
   "source": [
    "### Running the Environment\n",
    "\n",
    "To run the environment you need to instantiate the class, call reset to move the agent back to the start, then perform a series of actions to move the agent. For now let me move it manually, to make sure it is working, visualising the agent at each step. I also print the result of the step (the new state, reward, and terminal flag) for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-IuuVBjv_tc",
    "outputId": "03161fe6-1f82-456f-85f5-131474290516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "☝ This shows a simple visualisation of the environment state.\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "____o\n",
      "\n",
      "(Point(x=0, y=2), -1, False) ⬅ This displays the state and reward from the environment 𝐀𝐅𝐓𝐄𝐑 moving.\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "x___o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_x__o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "__x_o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "___xo\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____@\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Point(x=4, y=0), -1, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SimpleGridWorld(debug=True)\n",
    "print(\"☝ This shows a simple visualisation of the environment state.\\n\")\n",
    "s.step(Direction.SOUTH)\n",
    "print(s.step(Direction.SOUTH), \"⬅ This displays the state and reward from the environment 𝐀𝐅𝐓𝐄𝐑 moving.\\n\")\n",
    "s.step(Direction.SOUTH)\n",
    "s.step(Direction.SOUTH)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwvpSbUwngL-"
   },
   "source": [
    "#### Key Takeaways\n",
    "\n",
    "There are a few key lessons that you should commit to memory:\n",
    "\n",
    "- The **state** is an observation of the environment, which contains everything outside of the agent. For example, the agent's current position within the environment. In real world applications this could be the time of the day, the weather, data from a video camera, literally anything.\n",
    "- The **reward** fully specifies the optimal solution to the problem. In real life this might be profit or the number of new customers.\n",
    "- Every **action** mutates the state of the environment. This may or may not be observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y74EZUoSo60h"
   },
   "source": [
    "## A simple Reinforcement Learning Solution\n",
    "\n",
    "This simple algorithm we're about to implement operates by sampling the environment. In general, the idea is that if you can sample the environment enough times, you can begin to build a picture of the output, given any input. We can use this idea in RL. If we capture enough _trajectories_, where a trajectory is one full pass through an environment, then we can see which states are advantagous.\n",
    "\n",
    "To begin, we create a class that is capable of generating trajectories. Here, we pass in the environment, then, the `run` function repeatedly performs a step in the environment using a random action. Each step is stored in a list and return it to the user.\n",
    "\n",
    "> Note that this algorithm is known as the Monte Carlo method, which we will introduce in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "34Y8V-k89dFS"
   },
   "outputs": [],
   "source": [
    "class MonteCarloGeneration(object):\n",
    "  def __init__(self, env: object, max_steps: int = 1000, debug: bool = False):\n",
    "    self.env = env\n",
    "    self.max_steps = max_steps\n",
    "    self.debug = debug\n",
    "\n",
    "  def run(self) -> List:\n",
    "    buffer = []\n",
    "    n_steps = 0 # Keep track of the number of steps so I can bail out if it takes too long\n",
    "    state, _, _ = self.env.reset() # Reset environment back to start\n",
    "    terminal = False\n",
    "    while not terminal: # Run until terminal state\n",
    "      action = random.choice(self.env.action_space) # Random action. Try replacing this with Direction.EAST\n",
    "      next_state, reward, terminal = self.env.step(action) # Take action in environment\n",
    "      buffer.append((state, action, reward)) # Store the result\n",
    "      state = next_state # Ready for the next step\n",
    "      n_steps += 1\n",
    "      if n_steps >= self.max_steps:\n",
    "        if self.debug:\n",
    "          print(\"Terminated early due to large number of steps\")\n",
    "        terminal = True # Bail out if we've been working for too long\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly8Zmy5J-NFu"
   },
   "source": [
    "#### Visualising Trajectories\n",
    "\n",
    "As before, it's vitally important to visualise as much as possible, to gain an intuition into your problem. A simple first step is to view the agent's movement and trajectory. Here we severely limit the amount of exploration to save reams of output. Depending on your random seed you will see the agent stumbling around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymk3yN7H_AEU",
    "outputId": "036410de-2292-4806-a923-3b87ac85fcc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_x___\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "__x__\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "___x_\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "___x_\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "___x_\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "___x_\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "___x_\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "___x_\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "Terminated early due to large number of steps\n",
      "['⬇', '⬇', '⮕', '⮕', '⮕', '⬇', '⬆', '⬇', '⬆', '⬆']\n",
      "total reward: -10\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=True) # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env, max_steps=10, debug=True) # Instantiate the generation\n",
    "trajectory = generator.run() # Generate a trajectory\n",
    "print([t[1].value for t in trajectory]) # Print chosen actions\n",
    "print(f\"total reward: {sum([t[2] for t in trajectory])}\") # Print final reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2yDx2cnAS3G"
   },
   "source": [
    "### Quantifying Value\n",
    "\n",
    "There's an important quanity called the _action value function_. In summary, it is a measure of the value of taking a particular action, given all the experience. In other words, you can look at the previous trajectories, find out which of them lead to the highest values and look to use them again.\n",
    "\n",
    "To generate an estimate of this value, generate a full trajectory, then look at how far away the agent is from the terminal states at all steps.\n",
    "\n",
    "So this means we need a class to generate a full trajectory, from start to termination. That code is below. First we create a new class that accepts the generator from before; we'll use this later to generate the full trajectory.\n",
    "\n",
    "Then we create two fields to retain the experience observed by the agent. The first is recording the expected value at each state. This is the effectively the distance to the goal. The second is recording the number of times the agent has visited that state.\n",
    "\n",
    "Then we create a helper function to return a key for the dictionary (a.k.a. map) and an action value function to calculate the value of taking each action in each state. This is simply the average value over all visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4CSqugUJyR_3"
   },
   "outputs": [],
   "source": [
    "class MonteCarloExperiment(object):\n",
    "  def __init__(self, generator: MonteCarloGeneration):\n",
    "    self.generator = generator\n",
    "    self.values = defaultdict(float)\n",
    "    self.counts = defaultdict(float)\n",
    "\n",
    "  def _to_key(self, state, action):\n",
    "    return (state, action)\n",
    "  \n",
    "  def action_value(self, state, action) -> float:\n",
    "    key = self._to_key(state, action)\n",
    "    if self.counts[key] > 0:\n",
    "      return self.values[key] / self.counts[key]\n",
    "    else:\n",
    "      return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWH_Gxw6BVrW"
   },
   "source": [
    "Next we create a function to store this data after generating a full trajectory. There are several important parts of this function.\n",
    "\n",
    "The first is that we are using reversed trajectories. I.e. we start from the end and working backwards.\n",
    "\n",
    "The second is that we average the expected return over all visits. So this is reporting the value of an action, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iDXn0UmFBU6Z"
   },
   "outputs": [],
   "source": [
    "class MonteCarloExperiment(MonteCarloExperiment):\n",
    "  def run_episode(self) -> None:\n",
    "    trajectory = self.generator.run() # Generate a trajectory\n",
    "    episode_reward = 0\n",
    "    for i, t in enumerate(reversed(trajectory)): # Starting from the terminal state\n",
    "      state, action, reward = t\n",
    "      key = self._to_key(state, action)\n",
    "      episode_reward += reward  # Add the reward to the buffer\n",
    "      self.values[key] += episode_reward # And add this to the value of this action\n",
    "      self.counts[key] += 1 # Increment counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAFFF6ZhBUmZ"
   },
   "source": [
    "#### Running the Trajectory Generation\n",
    "\n",
    "Let's test this by setting some expectations. We're reporting the value of taking an action **on average**. So on average, you would expect the value of taking the `EAST` action when next to the terminal state would be -1, because it's right there, it's a single step and therefore a single reward of -1 to get to the terminal state.\n",
    "\n",
    "However, other directions will not be -1, because the agent will continue to stumble around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lz4m1mgpreTy",
    "outputId": "a2e69bbb-862e-48f6-cdd5-e2746d13a1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:  [0.0, -1.0, 0.0, 0.0]\n",
      "Run 1:  [-3.0, -1.0, -4.0, -12.0]\n",
      "Run 2:  [-3.0, -1.0, -4.0, -27.5]\n",
      "Run 3:  [-3.0, -1.0, -4.0, -27.5]\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=False) # Instantiate the environment - set the debug to true to see the actual movemen of the agent.\n",
    "generator = MonteCarloGeneration(env=env, debug=True) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(4):\n",
    "  agent.run_episode()\n",
    "  print(f\"Run {i}: \", [agent.action_value(Point(3,0), d) for d in env.action_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omKjnso-DdVW"
   },
   "source": [
    "So you can see from above that yes, when choosing east from the point to the west of the terminal state the expected return is -1. But notice that the agent (probably) did not observe that result straight away, because it takes time to randomly select it. (Run it a few more times to see what happens, you'll see random changes)\n",
    "\n",
    "#### Visualising the Value Function\n",
    "\n",
    "The value function (which we will refer to as \"state value function\" from now on) is the average expected return for all actions. In general, you should see that the value increases the closer you get to the goal. But because of the random movement, especially far away from the goal, there will be a lot of noise.\n",
    "\n",
    "Below we create a helper function to plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAkaVRETrt65",
    "outputId": "477d3a9e-2b27-405a-ba70-e5d0885c6a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-59.33 | -57.79 | -37.00 | -36.34 | -37.07 | \n",
      "-56.65 | -51.16 | -47.75 | -42.79 | -48.41 | \n",
      "-43.71 | -42.96 | -43.94 | -44.38 | -35.67 | \n",
      "-40.05 | -41.30 | -31.06 | -22.08 |  -8.62 | \n",
      "-38.38 | -30.30 | -14.50 |  -8.88 |    @   | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def state_value_2d(env, agent):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(env.height)):\n",
    "      for x in range(env.width):\n",
    "        if env.goal.x == x and env.goal.y == y:\n",
    "          res += \"   @  \"\n",
    "        else:\n",
    "          state_value = sum([agent.action_value(Point(x,y), d) for d in env.action_space]) / len(env.action_space)\n",
    "          res += f'{state_value:6.2f}'\n",
    "        res += \" | \"\n",
    "      res += \"\\n\"\n",
    "    return res\n",
    "\n",
    "print(state_value_2d(env, agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "armEvkb8EXux"
   },
   "source": [
    "### Generating Optimal Policies\n",
    "\n",
    "A _policy_ is a set of rules that an agent should follow. It is a strategy that works for that particular environment. You can now generate thousands of trajectories and track the expected value over time.\n",
    "\n",
    "With enough averaging, the expected values should present a clear picture of what the optimal policy is. See if you can see what it is?\n",
    "\n",
    "In the code below we are instantiating all the previous code and then generating 1000 episodes. Then we print out the state value function for every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "434JeYohWxUK",
    "outputId": "9f36603d-f7ce-4f73-be4a-4efc759d35a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 999\n",
      "-98.85 | -98.64 | -94.76 | -89.27 | -84.37 | \n",
      "-97.69 | -95.11 | -90.74 | -87.81 | -82.06 | \n",
      "-91.29 | -88.08 | -80.15 | -73.38 | -64.66 | \n",
      "-87.33 | -82.64 | -72.66 | -57.03 | -38.75 | \n",
      "-81.87 | -77.20 | -64.83 | -40.97 |    @   | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(1000):\n",
    "  clear_output(wait=True)\n",
    "  agent.run_episode()\n",
    "  print(f\"Iteration: {i}\")\n",
    "  # print([agent.action_value(Point(0,4), d) for d in env.action_space]) # Uncomment this line to see the actual values for a particular state\n",
    "  print(state_value_2d(env, agent), flush=True)\n",
    "  # time.sleep(0.1) # Uncomment this line if you want to see every episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSc-PczGE4b2"
   },
   "source": [
    "#### Plotting the Optimal Policy\n",
    "\n",
    "That's right! The optimal policy is to choose the action that picks the highest expected return. In other words, you want to move the agent towards regions of higher reward.\n",
    "\n",
    "Let me create another helper function to visualise where the maximal actions are pointing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udC_5ImwYBA1",
    "outputId": "1e96ab61-8b58-4df3-bf51-a3471bf50afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬆ | ⮕ | ⬇ | ⮕ | ⬇ | \n",
      "⬇ | ⬇ | ⬇ | ⬇ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
      "⬇ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⮕ | @ | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def argmax(a):\n",
    "    return max(range(len(a)), key=lambda x: a[x])\n",
    "\n",
    "def next_best_value_2d(env, agent):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(env.height)):\n",
    "      for x in range(env.width):\n",
    "        if env.goal.x == x and env.goal.y == y:\n",
    "          res += \"@\"\n",
    "        else:\n",
    "          # Find the action that has the highest value\n",
    "          loc = argmax([agent.action_value(Point(x,y), d) for d in env.action_space])\n",
    "          res += f'{env.action_space[loc].value}'\n",
    "        res += \" | \"\n",
    "      res += \"\\n\"\n",
    "    return res\n",
    "\n",
    "print(next_best_value_2d(env, agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsd37WzCFcSB"
   },
   "source": [
    "And there you have it. A policy. The above image spells out what the agent should do in each state. It should move towards regions of higher value. You can see (in general) that the arrows are all pointing towards the goal, as if by magic.\n",
    "\n",
    "For the arrows that are not, that is a more interesting story. The problem is that the agent is still entirely random at this point. It's stumbling around until it reachest the goal. The agent started in the top left, so on average, it takes a _lot_ of stumbling to find the goal.\n",
    "\n",
    "Therefore, for the points at the top left, furthest away from the goal, the agent will probably take many more random steps before it reachest the goal. In essence, it _doesn't matter which way the agent goes_. It will still take a long time to get there.\n",
    "\n",
    "Subsequent Monte Carlo algorithms fix this by updating the action value function every episode and using this knowledge to choose the action. So latter iterations of the agent are far more guided and intelligent.\n",
    "\n",
    "#### One Final Run\n",
    "\n",
    "To wrap this up, let's run the whole thing one more time. We will plot the state value function and the policy for all iteration steps. Watch how it changes over time. Add a sleep to slow it down to see it changing on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiCODK9C3qR9",
    "outputId": "b5189349-19fe-4d6a-883b-adee08557618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 999\n",
      "-106.92 | -104.87 | -99.46 | -91.93 | -88.39 | \n",
      "-103.63 | -102.25 | -96.18 | -88.39 | -82.27 | \n",
      "-101.76 | -96.87 | -90.96 | -82.10 | -71.96 | \n",
      "-94.89 | -88.36 | -77.60 | -61.89 | -41.65 | \n",
      "-93.03 | -85.19 | -72.19 | -45.81 |    @   | \n",
      "\n",
      "⬇ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
      "⬇ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
      "⬇ | ⬇ | ⬇ | ⬇ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⮕ | ⬇ | \n",
      "⮕ | ⮕ | ⮕ | ⮕ | @ | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(1000):\n",
    "  clear_output(wait=True)\n",
    "  agent.run_episode()\n",
    "  print(f\"Iteration: {i}\")\n",
    "  print(state_value_2d(env, agent))\n",
    "  print(next_best_value_2d(env, agent), flush=True)\n",
    "  # time.sleep(0.1) # Uncomment this line if you want to see the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R5gh9xjG1ZR"
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "You have now implemented your first reinforcement learning algorithm and be able to learn a policy that succesfully guides the agent to the goal position !\n",
    "\n",
    "Now, you can play around with the code to see how does the learning change if some aspects of the problem change. Here are some things that you can try:\n",
    "\n",
    "- Increase or decrease the size of the grid.\n",
    "- Add other terminating states\n",
    "- Change the reward to a different value\n",
    "- Change the reward to produce 0 reward per step, and a positive reward for the terminating state\n",
    "- Add a terminating state with a massive negative reward, to simulate a cliff\n",
    "- Add a hole in the middle\n",
    "- Add a wall\n",
    "- See if you can add the code to use the policy derived by the agent\n",
    "\n",
    "### Looking forward\n",
    "\n",
    "We see that the algorithm was able to solve the task. However, can we say that is has accurately learn the correct  value function and policy?\n",
    "\n",
    "If not, Can you propose some ideas in order to make the learning more efficient and accurate?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "201102_code_driven_rl_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
