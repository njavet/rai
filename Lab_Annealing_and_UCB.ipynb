{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnpMsVbWybsy"
   },
   "source": [
    "\n",
    "# Comparing Simple Exploration Techniques: ε-Greedy, Annealing, and UCB\n",
    "\n",
    "## E-Greedy and Bandit Algorithms\n",
    "\n",
    "Bandit algorithms provide a way to optimize single competing actions in the shortest amount of time. Imagine you are attempting to find out which advert provides the best click through rate of which button provides the most sales. You could show two ads and count the number of clicks on each, over a one week period. But this means that many people would see a sub-optimal ad. Instead, you can learn over time and progressively shift the shown advert towards the one that is performing better. For more information see the references in the book.\n",
    "\n",
    "This practice is based on the supplementary material of [Phil Widmer's book](https://rl-book.com/supplementary_materials/)\n",
    "\n",
    "## The Testing Environment\n",
    "\n",
    "In this workshop I will leverage a bandit library to simulate the behaviour and provide the algorithm that chooses an action. In essence, bandits generally use three forms of exploration. Standard ε-Greedy, which randomly chooses an action some proportion of the time, and annealing version, which reduces the exploration over time, and finally UCB, which chooses an action depending on how often the action has been sampled.\n",
    "\n",
    "## The Testing Environment\n",
    "\n",
    "First let me install the library and define the simulated behaviour of the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txx_umXnybsz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install banditsbook pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haa4-9mPybs0"
   },
   "outputs": [],
   "source": [
    "from arms.bernoulli import BernoulliArm\n",
    "\n",
    "# Define two adverts, with a probability of clicking from the users\n",
    "# This is a simulation. Imagine that these are real ads.\n",
    "arm0 = BernoulliArm(0.05)\n",
    "arm1 = BernoulliArm(0.4)\n",
    "arms = [arm0, arm1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kz1rCTIybs0"
   },
   "source": [
    "## Running the Experiment\n",
    "\n",
    "The code below will compare the three algorithms on the simulated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox-jGsBdybs0"
   },
   "outputs": [],
   "source": [
    "from arms.bernoulli import BernoulliArm\n",
    "from testing_framework.tests import test_algorithm\n",
    "from algorithms.epsilon_greedy.standard import EpsilonGreedy\n",
    "from algorithms.softmax.annealing import AnnealingSoftmax\n",
    "from algorithms.ucb.ucb1 import UCB1\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "num_sims = 1000 # Repetitions\n",
    "horizon = 250 # Number of steps in experiment\n",
    "n_arms = len(arms)\n",
    "\n",
    "algo1 = AnnealingSoftmax([], []) # Annealing ε-Greedy\n",
    "algo1.initialize(n_arms)\n",
    "algo2 = EpsilonGreedy(0.05, [], []) # Standard ε-Greedy, exploring 5% of the time\n",
    "algo3 = UCB1([], []) # UCB\n",
    "algo3.initialize(n_arms)\n",
    "algos = [(\"e_greedy\", algo2), (\"annealing_softmax\", algo1), (\"ucb\", algo3)]\n",
    "\n",
    "# A bit of code to loop over each algorithm and average the results\n",
    "df = pd.DataFrame()\n",
    "for algo in algos:\n",
    "    sim_nums, times, chosen_arms, rewards, cumulative_rewards = test_algorithm(\n",
    "        algo[1], arms, num_sims, horizon)\n",
    "    arrays = [sim_nums, times]\n",
    "    index = pd.MultiIndex.from_arrays(\n",
    "        arrays, names=('simulation', 'time'))\n",
    "    df_chosen_arm = pd.DataFrame(chosen_arms, index=index, columns=[algo[0]])\n",
    "    df_probability_selected = df_chosen_arm.groupby(level=1).sum() / num_sims\n",
    "    df = pd.concat([df, df_probability_selected], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lG-1JGj-ybs0"
   },
   "outputs": [],
   "source": [
    "df.plot(ylim=[0,1],ylabel=\"Probability of Optimal Action\",xlabel=\"Steps\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSAUVxlyybs0"
   },
   "source": [
    "You can see that the ε-Greedy algorithm is taking a long time to converge to a similar level of performance. The reason being that it is still spending a large proportion of the time chossing random actions.\n",
    "\n",
    "The annealing version rapidly reduces the amount of random exploration to speed this learning up. This is better, but you need to tune the hyper-parameters (initial exploration rate, final exploration rate and how fast to anneal) for your specific problem.\n",
    "\n",
    "UCB attempts to quantify the number of times that action/state has been explored. If it has been explored a lot, and it is not the best action, then there's little point in exploring more. This is good because there are no hyper-parameters but you'll need to store a representation\n",
    "UCB attempts to quantify the number of times that action/state has been explored. If it has been explored a lot, and it is not the best action, then there's little point in exploring more. This is good because there are no hyper-parameters but you'll need to store visitation counts; something that might not be possible for certain problems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
